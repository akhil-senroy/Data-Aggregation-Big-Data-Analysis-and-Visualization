{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name     :  Akhil Amit Senroy\n",
    "UBIT       :  akhilami <br>\n",
    "Person# :  50288588\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import csv \n",
    "from nytimesarticle import articleAPI\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "ConsumerKey = \"7nOjD4MCGqdpwvfgdGNs65m22\"\n",
    "ConsumerSecret = \"wpEO61KmNh3ddYfhULFsd2cdT8sFjYN6zlyQasCCqIzUIXl4OK\"\n",
    "AccessKey = \"464870798-75FP4gx8AO6Bil2Gk31Ds1x79BUlVFJVc6FQq0OO\"\n",
    "AccessSecret = \"vQXhHdINSjYEALuItjDuiDGuBkCWy7z6zSmzgUGlzDSKL\"\n",
    "auth = tweepy.OAuthHandler(ConsumerKey,ConsumerSecret)\n",
    "auth.set_access_token(AccessKey,AccessSecret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "nytapi = articleAPI('AJBvphX6dlpTN9T54u1XkAbg9cUjxA0r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "csvFile = open('data/tw/sample/twpiano.csv', 'w')\n",
    "csvWriter = csv.writer(csvFile)\n",
    "\n",
    "for tweet in tweepy.Cursor(api.search,q=\" piano -filter:retweets\",\\\n",
    "                           lang=\"en\",\\\n",
    "                           count = 2000,\\\n",
    "                           include_entities=True,\\\n",
    "                           since_id=2019-1-1).items():\n",
    "    if not tweet.retweeted:\n",
    "        print(tweet.created_at, tweet.text)\n",
    "        csvWriter.writerow([tweet.created_at, tweet.text.encode('utf-8')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "colnames = ['date', 'tweets']\n",
    "data = pd.read_csv('data/tw/sample/twsong.csv', names=colnames,index_col=False)\n",
    "col = data['tweets']\n",
    "txt = col.to_string()\n",
    "txt_file = open('data/tw/sample/twsong.txt','w')\n",
    "txt_file.write(txt)\n",
    "txt_file.close()\n",
    "# saved_column = df.column_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New York Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import time \n",
    "\n",
    "linksSet = {\"\"}\n",
    "topics = [\"music\",\"song\",\"guitar\",\"violin\",\"piano\",\"sing\",\"orchestra\"]\n",
    "\n",
    "\n",
    "for topic in topics:\n",
    "    num=1\n",
    "    for pageno in range(0,20):\n",
    "#         print(topic)\n",
    "        articles = nytapi.search(q = topic, begin_date = 20190101 , page = pageno )\n",
    "        if 'response' in articles:\n",
    "            n = len(articles['response']['docs'])\n",
    "            for j in range(n):\n",
    "                url = articles['response']['docs'][j]['web_url']\n",
    "                linksSet.add(url)\n",
    "#                 print(url)\n",
    "                try:\n",
    "                    data = requests.get(url)\n",
    "                    soup = BeautifulSoup(data.content, 'html.parser')\n",
    "                    prettySoup = soup.prettify()\n",
    "                    f = open('data/nyt/sample/'+topic+str(num)+'.txt', 'w')\n",
    "                    for k in range((len(soup.find_all('p')))-3):\n",
    "                        f.write(soup.find_all('p')[k].get_text())\n",
    "                    f.close()\n",
    "                except:\n",
    "                    continue\n",
    "                num+=1\n",
    "                \n",
    "\n",
    "    time.sleep(10)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CommonCrawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CommonCrawlEnv/links.txt\") as input:\n",
    "    links = input.read()\n",
    "    num=0\n",
    "    for link in links.split(\"\\n\\n\"):\n",
    "        try:\n",
    "            data = requests.get(link)\n",
    "            soup = BeautifulSoup(data.content, 'html.parser')\n",
    "            prettySoup = soup.prettify()\n",
    "            f = open('data/cc/sample/cc'+str(num)+'.txt', 'w')\n",
    "            for k in range((len(soup.find_all('p')))-3):\n",
    "                f.write(soup.find_all('p')[k].get_text())\n",
    "            f.close()\n",
    "        except:\n",
    "            continue\n",
    "        num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from os.path import join, getsize\n",
    "\n",
    "read_files = glob.glob('data/cc/sample/cc*')\n",
    "for f in read_files:\n",
    "    if getsize(f)==0:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stopwords=set(stopwords.words('english'))\n",
    "# https://stackoverflow.com/questions/17749058/combine-multiple-text-files-into-one-text-file-using-python\n",
    "import glob\n",
    "i=0\n",
    "read_files = glob.glob(\"data/cc/sample/*\")\n",
    "\n",
    "for f in read_files:\n",
    "    with open(\"data/cc/clean_sample/cleanCC\"+str(i)+\".txt\", \"w+\") as newf:\n",
    "        i+=1\n",
    "        with open(f, \"r\") as infile:\n",
    "            file=infile.read()\n",
    "    \n",
    "            words=word_tokenize(file)\n",
    "\n",
    "            # include only words no numerical or special character\n",
    "            words_rem_punct=[word for word in words if word.isalpha()]\n",
    "\n",
    "            # removing all the stop words \n",
    "            meaningful_words=[x for x in words_rem_punct if not x in stopwords]\n",
    "            meaningful_words[:10]\n",
    "\n",
    "            #removing words with only one character since they will add any meaning to the data\n",
    "            meaningful_words=[z for z in meaningful_words if not len(z)==1]\n",
    "            # meaningful_words[:10]\n",
    "\n",
    "            # stemming of the data(removing words that convey the same meaning)\n",
    "            stemmer=PorterStemmer()\n",
    "            stemmed_words=[stemmer.stem(words) for words in meaningful_words]\n",
    "            # print(stemmed_words)\n",
    "\n",
    "            #writting stemmed words into a file\n",
    "\n",
    "            for w in stemmed_words:\n",
    "                newf.write(w.lower()+\" \")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Multiple Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/17749058/combine-multiple-text-files-into-one-text-file-using-python\n",
    "import glob\n",
    "\n",
    "read_files = glob.glob(\"data/cc/outputWordCoOccur/p*\")\n",
    "\n",
    "with open(\"data/cc/outputWordCoOccur/combined_output.txt\", \"wb\") as outfile:\n",
    "    for f in read_files:\n",
    "        with open(f, \"rb\") as infile:\n",
    "            outfile.write(infile.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Text files to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('data/cc/outputWordCoOccur/combined_output.txt', sep=\"\\t\", header=None)\n",
    "data.to_csv('data/cc/outputWordCoOccur/combined_output.csv', sep=',',index=False,header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort in Descending order of Max Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('data/cc/outputWordCoOccur/combined_output.csv', \n",
    "          mode='rt') as f, open('data/cc/WordCoOccur.csv',\n",
    "                                'w') as final, open('data/cc/toptenWordCoOccur.csv','w') as topten:\n",
    "    writer = csv.writer(final)\n",
    "    writer2 = csv.writer(topten)\n",
    "    reader = csv.reader(f)\n",
    "    _ = next(reader)\n",
    "    sortedlist = sorted(reader, key=lambda row: int(row[1]),reverse=True)\n",
    "    for row in sortedlist:\n",
    "        writer.writerow(row)\n",
    "    for row in sortedlist[:10]:\n",
    "        writer2.writerow(row)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br> \n",
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sing|one\\t1\\n', 'music|close\\t1\\n', 'song|hit\\t1\\n', 'song|hit\\t1\\n', 'song|flute\\t1\\n', 'sing|have\\t1\\n', 'sing|feel\\t1\\n', 'orchestra|start\\t1\\n', 'guitar|piano\\t1\\n', 'piano|hit\\t1\\n', 'piano|burst\\t1\\n', 'music|familiar\\t1\\n', 'music|close\\t1\\n', 'guitar|sound\\t1\\n']\n"
     ]
    }
   ],
   "source": [
    "#http://www.science.smith.edu/dftwiki/index.php/Hadoop_Tutorial_2_--_Running_WordCount_in_Python\n",
    "#import sys\n",
    "#for line in sys.stdin:\n",
    "topics = [\"music\",\"song\",\"guitar\",\"violin\",\"piano\",\"sing\",\"orchestra\"]\n",
    "\n",
    "line = \" first report peopl magazin the actress musician wed last juli upstat new york thi first marriag bob dylan one incorpor stage banter recent year he rare even speak introduc band but chang last night someon broke photo polici show vienna austria after sing one vers blowin wind dylan stop said audienc take pictur take pictur we either play pose okay befor there point mani young rapper live becom infatu mf doom joey badass mine volum special herb compil beat debut album milo scour fileshar servic kazaa find doom loosi year odd futur compatriot tyler the creator earl sweatshirt look like toddler meet mall santa came the world readi new carli rae jepsen album it four long year sinc although gotten cut to the feel plenti collab meantim but still world readi music close carli rae jepsen album thank exactli world get earlier month jepsen announc new album call dedic have song hit seen bitch play flute song hit the shoot it reason question it also relev question accompani footag woman pose first bust song flute solo break biggest viral danc craze backup dancer what gloriou sight behold here sing have ever seen bitch rosalía still play club heroin sing feel readi arena for first american club show worldwid el mal querer tour first ever north play headlin gig hit stage mayan lo angel show held wednesday two appear coachella place size muic may in the number one review everi singl singl histori billboard orchestra start chart begin work way present grand funk we an guitar piano hit septemb stay at week they realli american band they may american band grand funk railroad piano burst flint michigan la disput rut after play togeth give take member close year experimentalist stuck music familiar come idea upcom album in search someth anyth songwrit process la disput rut after play togeth give take music close year experimentalist stuck routin familiar come idea upcom album in search someth anyth songwrit process admittedli name intellexu bit hear the record good like realli good you expect much nico segal nate fox guy play key role craft chanc the rapper guitar sound count frank ocean kany west ultralight beam admittedli name intellexu bit hear\"\n",
    "#--- split the line into words ---\n",
    "words = line.strip().split()\n",
    "i=0\n",
    "data=[]\n",
    "# f=open('mapperTrial.txt','w')\n",
    "    #--- output tuples [word, 1] in tab-delimited format---\n",
    "for i in range(len(words)-1):\n",
    "    if words[i] in topics:\n",
    "    #     print('%s\\t%s' % (word, \"1\"))\n",
    "        w = words[i]+\"|\"+words[i+1]+\"\\t1\\n\"\n",
    "        data.append(w)\n",
    "    #     f.write(w)\n",
    "    #     print(w.strip())\n",
    "# f.close()\n",
    "    \n",
    "    \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "song|hit\t2\n",
      "music|close\t2\n",
      "song|flute\t1\n",
      "sing|one\t1\n",
      "sing|have\t1\n",
      "sing|feel\t1\n",
      "piano|hit\t1\n",
      "piano|burst\t1\n",
      "orchestra|start\t1\n",
      "music|familiar\t1\n",
      "guitar|sound\t1\n",
      "guitar|piano\t1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    " \n",
    "# maps words to their counts\n",
    "word2count = {}\n",
    " \n",
    "# input comes from STDIN\n",
    "for line in data:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    " \n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    # convert count (currently a string) to int\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        word2count[word] += count\n",
    "    except:\n",
    "        word2count[word] = count\n",
    " \n",
    " # write the tuples to stdout\n",
    " # Note: they are unsorted\n",
    "# for word in word2count.keys():\n",
    "#     print('%s\\t%s'% ( word, word2count[word] ))\n",
    "    \n",
    "word2count= sorted(word2count.items(), key = \n",
    "             lambda kv:(kv[1], kv[0]),reverse=True)\n",
    "\n",
    "for word in word2count:\n",
    "    print('%s\\t%s'% ( word[0], word[1] ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
